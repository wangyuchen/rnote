---
title: Logistic Regression (Single-Trial Data)
---

```{r, echo=FALSE}
library(knitr)
library(ggplot2)
```

The relationship between countries’ credit ratings and the volatility of the countries’ stock markets was examined in the _Journal of Portfolio Management_ (Spring 1996). Our interest lies in whether volatility (standard deviation of stock returns) and credit rating (in percent) can be used to classify a country into one of the two market types (developed or emerging).

## Descriptive analysis
The data is read into a `data.frame` with the response as a factor. It's customary and preferrable to use a classification variable as a factor and let R do the rest of the things. When the factor is created from character data like we do in this example, the default order of the levels of that factor are alphabetical. It can be changed so that the level in higher order are coded with larger number. In `glm()`, it always model the level with higher order as the event. Therefore here we changed the order of levels so that in glm we can model the probability of the response being a developed country.

```{r}
volatile <- read.table("datasets/volatile.dat")
names(volatile) <- c('country/region', 'volatile', 'credit', 'market')
volatile$market <- factor(volatile$market, levels = c('emerge', 'develop'))
kable(volatile)
```


## Logistic regression
In R, logistic regression model is fitted by `glm()` with binomial distribution family. The default link function for binomial family is the logit link.

```{r}
fit <- glm(market ~ volatile + credit, family = "binomial", data = volatile)
fit
```

### Model fit results
```{r}
# response profile
summary(volatile$market)
```

In order to compute the model fit statistics for an intercept only model, we can just fit the model explicitely and use the generic functions.

```{r}
fit2 <- glm(market ~ 1, family = "binomial", data = volatile)
AIC(fit2, fit)
BIC(fit2, fit)
-2 * logLik(fit2)
-2 * logLik(fit)
```

In the SAS output, the same model is fitted using `PROC GENMOD`. There are some additional output accessing goodness of fit. Here we compute some of the statistics.

```{r}
# Deviance
fit$deviance
fit$df.residual
fit$deviance / fit$df.residual

# Pearson's Chi-Square
sum(residuals(fit, "pearson")^2)
sum(residuals(fit, "pearson")^2) / fit$df.residual
```

These all show evidence of under-dispersion.

### Hypothesis testing
We know the likelihood ratio test is just the $-2log\Lambda$, we can use the information above to calculate it.

```{r}
-2 * as.numeric(logLik(fit2) - logLik(fit))
1 - pchisq(-2 * as.numeric(logLik(fit2) - logLik(fit)), df = 2)  # p-value
```

For Wald test $H_0: L\theta = 0$, we need to specify L to compute the test statistic. For example if we want to test $H_0: \beta_1 = \beta_2 = 0$, we can set L in the following way:

```{r}
L <- rbind(c(0, 1, 0), c(0, 0, 1))
L
```

There is a function `wald.test` from the _aod_ package. Note that for a Wald test, we need the variance of the parameter estimates, which is $a(\phi)(\hat{F}\hat{V}\hat{F})^{-1}$. This matrix can be calculated using the `vcov()` function. It's calculated by QR decomposition, so the result is slightly different than those from SAS.

```{r}
library(aod)
wald.test(vcov(fit), b = coef(fit), L = L)
```

In SAS, the analysis of maximum likelihood estimates is done by Wald test and a Chi-square distribution with degree of freedom 1. This is the same as the a Z-test with a standard normal distribution. The Z statistic is the square root of the Wald Chi-square statistic and the p-values are the same.

### Concordance

The concordant and discordant percent all requires the fitted value to be calculated. Note that when we are modeling the probability for `develop` because it is the second level in the variable `market`.
```{r}
i <- volatile$market == 'develop'
j <- volatile$market == 'emerge'
pairs <- sum(i) * sum(j)

# expand pairs into grid
grid <- with(volatile, 
             cbind(expand.grid(yi = market[i], yj = market[j]),
                   expand.grid(pi = fitted(fit)[i], pj = fitted(fit)[j])))

# Percent Concordant
sum(grid$pi > grid$pj) / pairs

# Percent Discordant
sum(grid$pi < grid$pj) / pairs

# Somers' D
nc <- sum(grid$pi > grid$pj)
(nc - (pairs - nc)) / pairs

# Tau-a 
2 * (nc - (pairs - nc)) / (nobs(fit) * (nobs(fit) - 1))
```


### Parameter estimates and confidence intervals

```{r}
summary(fit)$coefficients
```

The estimates of odds ratio is obtained by applying the exponential function to the parameter estimates and confidence intervals.

```{r, warning=F}
# profile likelihood confidence interval
confint(fit)
exp(cbind(OR = coef(fit)[-1], confint(fit)[-1, ]))  # odds ratio
```


```{r}
# Wald confidence intervals
confint.default(fit)

exp(coef(fit)[-1])  # odds ratio
exp(confint.default(fit)[-1, ])
```


### Classification table

SAS output have a nice classification table that shows the classification results under different cutoffs. Thanks to [this answer](http://stats.stackexchange.com/a/4873), R can achieve a similar table as well, though there is a little bit difference due to precision (the maximum absolute difference between the two predicted probabilities are 1.890711e-08). 

```{r, results=3}
getMisclass <- function(cutoff, p, event) {
   pred <- factor(1*(p > cutoff), labels = levels(event)) 
   t <- table(pred, event)
   cat("cutoff ", cutoff, ":\n")
   print(t)
   cat("correct    :", round(sum(t[c(1,4)])/sum(t), 3),"\n")
   cat("Specificity:", round(t[1]/sum(t[,1]), 3),"\n")
   cat("Sensitivity:", round(t[4]/sum(t[,2]), 3),"\n\n")
   invisible(t)
}
cutoffs <- seq(0.1, .9, by=.1)
getMisclass(.5, p = fitted(fit), event = volatile$market)
```
You can apply this function to get a more thorough output like SAS.

```{r, eval=F}
sapply(cutoffs, getMisclass, p=fitted(fit), event=volatile$market)
```


















